Simplified Explanation of the Whole Process (Local vs. GitHub):
1. How it runs LOCALLY (your machine):
Setup (One-time or when needed):
You have your code (src/, tests/, Dockerfile, etc.).
You install Python packages from requirements.txt (including dvc[gcs], mlflow, flask, etc.) into your local Conda environment.
You configure DVC to use your GCS bucket as a remote (in .dvc/config).
You set GOOGLE_APPLICATION_CREDENTIALS to point to your gcp-creds2.json.
You start your local MLflow server (mlflow server ... --port 5002).
You set export MLFLOW_TRACKING_URI="http://localhost:5002" in your terminal.
Training (dvc repro):
dvc repro reads dvc.yaml.
It sees the train_model stage depends on src/train.py, src/preprocess.py, and data/raw/reviews.csv.dvc.
If data/raw/reviews.csv isn't in your workspace or local DVC cache, DVC (using GOOGLE_APPLICATION_CREDENTIALS) pulls it from your GCS DVC remote.
DVC runs python src/train.py.
train.py (because MLFLOW_TRACKING_URI is http://localhost:5002):
Connects to your local MLflow server.
Logs parameters, metrics, and artifacts (model, vectorizer, plot) to this server.
Registers the trained sentiment_model.joblib to the MLflow Model Registry on this local server.
Also saves sentiment_model.joblib and tfidf_vectorizer.joblib to your local models/ directory (as defined in dvc.yaml outputs).
dvc repro updates dvc.lock with the hashes of the new models/*.joblib files.
Pushing DVC Data (dvc push):
dvc push (using GOOGLE_APPLICATION_CREDENTIALS) uploads the content of any new/changed DVC-tracked files (like the models/*.joblib if their content changed, and data/raw/reviews.csv if it was new to the remote) from your local .dvc/cache/ to your GCS DVC remote.
Running the API Locally (python src/app.py):
Ensure MLFLOW_TRACKING_URI="http://localhost:5002" is exported in this terminal.
app.py starts.
It tries to load the main model from models:/SentimentAnalysisModelIMDB/Staging via your local MLflow server. (You'd have manually transitioned a version to "Staging" in the MLflow UI).
It loads tfidf_vectorizer.joblib from your local models/ directory.
The API runs, ready to serve predictions.
Building Docker Image Locally (docker build ...):
Dockerfile COPY src /app/src and COPY requirements.txt . etc.
COPY models /app/models copies the sentiment_model.joblib and tfidf_vectorizer.joblib that were generated by dvc repro into the image.
When this image runs as a container, app.py inside it will try to connect to an MLFLOW_TRACKING_URI (if passed as an env var to the container). If not, it falls back to using the sentiment_model.joblib from the copied models/ directory.
2. How it runs when PUSHED TO GITHUB (GitHub Actions CI/CD):
Trigger: You git push origin main (with your latest code, dvc.lock, ci.yml).
Job 1: build_and_test
Runner starts (fresh Ubuntu environment).
Checks out your Git repository (code, dvc.lock, dvc.yaml, requirements.txt, etc.).
Sets up Python 3.9.
Installs all Python packages from requirements.txt.
"Set up DVC and Pull Data from GCS" step:
Sets GOOGLE_APPLICATION_CREDENTIALS using secrets.GCP_SA_KEY.
Runs dvc pull data/raw/reviews.csv -v. DVC connects to your GCS DVC remote and downloads reviews.csv into the runner's workspace and DVC cache.
"Reproduce DVC pipeline" step:
MLFLOW_TRACKING_URI is set to "" (empty string).
Runs dvc repro -v.
src/train.py runs.
Since MLFLOW_TRACKING_URI is empty, train.py (if modified to handle this) might try to log to a local temporary mlruns folder on the runner OR log errors about MLflow connection but crucially it must still save sentiment_model.joblib and tfidf_vectorizer.joblib to the models/ directory because these are outs in dvc.yaml.
dvc repro updates dvc.lock on the runner (this change isn't committed back to your repo from CI unless you add steps to do so, which is advanced).
"Lint with Flake8" step: Runs flake8 src/. Fails job if lint issues.
"Run Tests with Pytest" step: Runs pytest tests/ -v. Fails job if tests fail.
Job 2: build_docker_and_push_to_ar (if build_and_test passed and it's a push to main)
Runner starts (another fresh Ubuntu environment).
Checks out your Git repository again.
Authenticates to GCP using google-github-actions/auth with secrets.GCP_SA_KEY.
Sets up Python 3.9.
"Set up DVC and Pull Models for Docker Build" step:
Installs dependencies from requirements.txt.
Sets GOOGLE_APPLICATION_CREDENTIALS using secrets.GCP_SA_KEY (via temp file).
Runs dvc pull models/sentiment_model.joblib models/tfidf_vectorizer.joblib -v. DVC uses the dvc.lock from your checked-out repository and your GCS DVC remote to download the correct versions of these two files (which you pushed from local after your successful local dvc repro) into the runner's DVC cache and then checks them out into the models/ directory in the runner's workspace.
Verifies these files exist and are not empty.
Sets up Docker Buildx.
Configures Docker to auth with Artifact Registry (gcloud auth configure-docker ...).
"Build and push Docker image to Artifact Registry" step:
docker build ... runs using your Dockerfile.
The COPY models /app/models in your Dockerfile copies the sentiment_model.joblib and tfidf_vectorizer.joblib (that were just pulled by DVC into the workspace) into the Docker image.
The image is tagged and pushed to your Google Artifact Registry.
Job 3: deploy_to_cloud_run (if build_docker_and_push_to_ar passed)
Runner starts.
Authenticates to GCP.
"Deploy to Google Cloud Run" step:
Pulls the Docker image (that was just pushed, tagged with commit SHA) from your Google Artifact Registry.
Deploys this image as a new revision to your Cloud Run service.
It passes the MLFLOW_TRACKING_URI=${{ secrets.MLFLOW_SERVER_URI_PROD || '' }} environment variable to the Cloud Run service.
If secrets.MLFLOW_SERVER_URI_PROD is set to a real, deployed MLflow server, your app in Cloud Run will try to use it.
If secrets.MLFLOW_SERVER_URI_PROD is empty or not set, your app.py in Cloud Run will get an empty MLFLOW_TRACKING_URI, log a warning, and then use the fallback mechanism to load sentiment_model.joblib from the /app/models/ directory inside the container (which was copied during Docker build). The tfidf_vectorizer.joblib is also loaded from /app/models/.


--------------

1. Create a GCS Bucket for MLflow Artifacts:
* Go to GCP Console -> Cloud Storage -> Buckets.
* Click "CREATE BUCKET".
* Name: e.g., sentiment-mlops-mlflow-artifacts (must be globally unique).
* Location type: Region, select your preferred region (e.g., europe-west1).
* Storage class: Standard.
* Access control: Uniform.
* Click "CREATE".
* Your goal: Successfully create the bucket. Note down its name (gs://your-bucket-name).
2. Create a Cloud SQL for PostgreSQL Instance:
* Go to GCP Console -> SQL.
* Click "CREATE INSTANCE".
* Choose "PostgreSQL".
* Instance ID: e.g., mlflow-db-instance.
* Password: Set a strong password for the default postgres user. SAVE THIS PASSWORD SECURELY.
* Database version: e.g., PostgreSQL 13, 14, or 15.
* Region: Same as your GCS bucket and other services (e.g., europe-west1).
* Machine type: db-f1-micro or db-g1-small (Shared core) is fine for now.
* Connections (Important for initial setup):
* Check "Public IP".
* Under "Authorized networks", click "ADD NETWORK".
* Name: my-local-ip (or similar).
* Network: Enter your current public IP address followed by /32. (Search "what is my IP" on Google to find it). This allows your current machine to connect to the database later if needed for setup or direct inspection, and it's also useful if the GCE VM initially tries to connect over public IP before Cloud SQL Proxy is fully effective.
* Click "CREATE INSTANCE". This will take several minutes.
* Your goal: Instance created successfully.
3. Once the Cloud SQL Instance is "Running":
* Click on the instance name.
* Note down its Instance Connection Name (from the Overview tab, looks like PROJECT_ID:REGION:INSTANCE_ID).
* Go to the "Databases" tab (on the left of the instance details page).
* Click "CREATE DATABASE".
* Database name: mlflow_backend_db
* Click "CREATE".
* (Optional, but good practice for later) Go to the "Users" tab.
* Click "CREATE USER ACCOUNT".
* User name: mlflow_user
* Password: Create a strong password. SAVE THIS SECURELY.
* Click "CREATE".
* Your goal: mlflow_backend_db database exists. mlflow_user (optional) exists.